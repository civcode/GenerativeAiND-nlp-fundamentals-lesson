{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import string\n",
    "import re\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Can be multiple actions to normalize text\n",
    "    # Only keep ASCII letters, numbers, punctuation, and whitespace characters\n",
    "    # Evquivalent to string.printable\n",
    "    acceptable_characters = (\n",
    "        string.ascii_letters\n",
    "        + string.digits\n",
    "        + string.punctuation\n",
    "        + string.whitespace\n",
    "    )\n",
    "    normalized_text = ''.join(\n",
    "        filter(lambda letter: letter in acceptable_characters, text)\n",
    "    )\n",
    "    # Make text lower-case\n",
    "    normalized_text = normalized_text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # Character-based\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # Split based on spaces\n",
    "    smaller_pieces = text.split()\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps before breaking things further\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    # Apply created steps \n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: list[str] = pretokenize_text(normalized_text)\n",
    "    tokens = []\n",
    "    # Go through small pieces to make full tokens\n",
    "    for word in pretokenized_text:\n",
    "        tokens.extend(\n",
    "            re.findall(\n",
    "                f'[\\w]+|[{string.punctuation}]', # Split word at punctuations \n",
    "                word,\n",
    "            )\n",
    "        )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for some tasks\n",
    "def postprocess_tokens(tokens: list[str]) -> list[str]:\n",
    "    # Add beginning and end of sequence tokens\n",
    "    bos_token = '##BOS##'\n",
    "    eos_token = '##EOS##'\n",
    "    updated_tokens = (\n",
    "        [bos_token]\n",
    "        + tokens\n",
    "        + [eos_token]\n",
    "    )\n",
    "    return updated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding: Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try out our full tokenization process! Let's use this sample text to\n",
    "see how our tokenization pipeline handles it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Louis continued to say, \"Penguins are important, \n",
      "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn\\'t forget the nuumber 1 priority: the READER!\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = '''Mr. Louis continued to say, \"Penguins are important, \n",
    "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
    "'''\n",
    "\n",
    "print(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##BOS##', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '##EOS##']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to encode the tokens to IDs we'll give to a model. But first we need\n",
    "to define how to map each token to a unique ID. An easy method can be to\n",
    "arbitrarily count the unique tokens from our corpus.\n",
    "\n",
    "We'll use the following as our sample corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally this would be much bigger\n",
    "sample_corpus = (\n",
    "    '''Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn't forget the nuumber 1 priority: the READER!\"''',\n",
    "    '''BRUTUS:\\nHe's a lamb indeed, that baes like a bear.''',\n",
    "    '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve unique tokens (from the pipeline defined above) in a set\n",
    "unique_tokens = set()\n",
    "for text in sample_corpus:\n",
    "    tokens_from_text = tokenize_text(text)\n",
    "    tokens_from_text = postprocess_tokens(tokens_from_text)\n",
    "    unique_tokens.update(tokens_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping (dictionary) for unique tokens using arbitrary & unique IDs\n",
    "token2id = {\n",
    "    token: idx\n",
    "    for idx, token in enumerate(unique_tokens)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, create a mapping for IDs to convert back to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {idx: token for token, idx in token2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our encoder and decoder to transform our tokens to IDS and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens: list[str]) -> list[int]:\n",
    "    # Note this doesn't handle tokens not mapped\n",
    "    encoded_tokens = [\n",
    "        token2id[token]\n",
    "        for token in tokens\n",
    "    ]\n",
    "    return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int]) -> list[str]:\n",
    "    token_strings = [\n",
    "        id2token[idx]\n",
    "        for idx in ids\n",
    "    ]\n",
    "    return token_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['##BOS##', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '##EOS##']\n",
      "\n",
      "Encoded Tokens:\n",
      "[33, 43, 57, 45, 31, 41, 32, 4, 20, 14, 21, 9, 4, 46, 35, 47, 5, 39, 49, 58, 38, 51, 53, 26, 58, 6, 22, 20, 8]\n",
      "\n",
      "Decoded Tokens:\n",
      "['##BOS##', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '##EOS##']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing out encoding and decoding \n",
    "sample_text = '''Mr. Louis continued to say, \"Penguins are important, \n",
    "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
    "'''\n",
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "\n",
    "print(f'Tokens:\\n{tokens}\\n')\n",
    "\n",
    "encoded_tokens = encode(tokens)\n",
    "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')\n",
    "\n",
    "decoded_tokens = decode(encoded_tokens)\n",
    "print(f'Decoded Tokens:\\n{decoded_tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hugging Face Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is an important step in most NLP tasks. Hugging Face has been an\n",
    "invaluable resource in training, using, and sharing different tokenizers!\n",
    "\n",
    "The API is flexible where you can use a tokenizer off the shelf, fine-tune a\n",
    "tokenizer with your own data, or even train your own completely from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore Hugging Face's tokenizers by using a pretrained\n",
    "model. Hugging Face has many tokenizers available that have already been trained\n",
    "for specific models and tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "my_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding: Text to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple ways to get the tokens from text. The simplest is calling\n",
    "`.tokenize()` on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!']\n"
     ]
    }
   ],
   "source": [
    "raw_text = '''Rory\\'s shoes are magenta and so are Corey\\'s but they aren\\'t nearly as dark!'''\n",
    "tokens = my_tokenizer.tokenize(raw_text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another is calling the tokenizer with the text and then calling the `.tokens()`\n",
    "method.\n",
    "\n",
    "This will also return some special tokens depending on the pretrained tokenizer\n",
    "used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "detailed_tokens = my_tokenizer(raw_text).tokens()\n",
    "\n",
    "print(detailed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the tokens as integer IDs, there are again a few methods.\n",
    "\n",
    "The first is using the tokenizers `.encode()` method on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.encode(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also the `.convert_tokens_to_ids()` tokenizer method if we already have\n",
    "the tokens (as strings) to get the IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n",
      "[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "print(detailed_tokens)\n",
    "detailed_ids = my_tokenizer.convert_tokens_to_ids(detailed_tokens)\n",
    "print(detailed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way can look a little complex but can be useful when working with\n",
    "tokenizers for certain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first call the tokenizer on the text like we did last time but with no extra \n",
    "method.\n",
    "\n",
    "This returns an object that has a few different keys available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then focus on `input_ids` which are the IDs associated with the\n",
    "tokenizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer(raw_text).input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding: Tokens to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We of course can use the tokenizer to go from token IDs to tokens and back to text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the inverse of the `.enocde()` method: `.decode()`!\n",
    "\n",
    "This will get us a full string using the tokens (from the IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Rory's shoes are magenta and so are Corey's but they aren't nearly as dark! [SEP]\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = my_tokenizer.encode(raw_text)\n",
    "my_tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might've noticed that you didn't go back to the original text and instead\n",
    "have some special tokens. This is because the `.encode()` will return IDs for\n",
    "special tokens (assuming the pretrained tokenizer used special tokens).\n",
    "\n",
    "To use `.decode()` but not the special tokens, we can use the parameter\n",
    "`skip_special_tokens` and set it to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rory's shoes are magenta and so are Corey's but they aren't nearly as dark!\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead wanted a list of the tokens decoded from the IDs, we could instead\n",
    "use the `.convert_ids_to_tokens()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on the Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to consider is if a string is outside of the tokenizer's vocabulary,\n",
    "also know as an \"unkown\" token. They are typically represented with `[UNK]` or\n",
    "some other similar variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the tokenizer encoded the text so each character was a token (which is\n",
    "actually not as easy as it sounds), then it would be impossible to have an\n",
    "\"unknown\" token. Word-based tokenization will always be in danger of having \n",
    "\"unknown\" tokens since it's virtually impossible to have every possible word (\n",
    "and \"non-word\") in its vocabulary!\n",
    "\n",
    "And so you might think that subword tokenization wouldn't have an issue with\n",
    "\"unknown\" tokens. And although there are fewer than word-based tokenization, it\n",
    "does happen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers are specific so it's important to use a tokenizer that will recognize\n",
    "most of the text you're working with! For example, a lot of tokenizers might not\n",
    "consider emoji as tokens but could be really important if emoji are especially\n",
    "numerous in your data (like a corpus of chat messages)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🥱 the dog next door kept barking all night!!\n",
      "['[CLS]', '[UNK]', 'the', 'dog', 'next', 'door', 'kept', 'barking', 'all', 'night', '!', '!', '[SEP]']\n",
      "[CLS] [UNK] the dog next door kept barking all night!! [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = '🥱 the dog next door kept barking all night!!'\n",
    "ids = my_tokenizer.encode(phrase)\n",
    "print(phrase)\n",
    "print(my_tokenizer.convert_ids_to_tokens(ids))\n",
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow my dad thought mcdonalds sold tacos ☠\n",
      "['[CLS]', 'w', '##ow', 'my', 'dad', 'thought', 'm', '##c', '##don', '##ald', '##s', 'sold', 'ta', '##cos', '[UNK]', '[SEP]']\n",
      "[CLS] wow my dad thought mcdonalds sold tacos [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = '''wow my dad thought mcdonalds sold tacos \\N{SKULL AND CROSSBONES}'''\n",
    "ids = my_tokenizer.encode(phrase)\n",
    "print(phrase)\n",
    "print(my_tokenizer.convert_ids_to_tokens(ids))\n",
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're seeing a lot of \"unknown\" tokens with the text you're working with,\n",
    "might consider using a different tokenizer appropiate for the task. Or it's also\n",
    "possible to fine-tune a pretrained model or train one from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Properties of Hugging Face's Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of great features when using tokenizers in Hugging Face that can make it very simple to try out and use different models. Here we'll breifly discuss some properties that can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load a couple different models:\n",
    "\n",
    "* `bert-base-cased` ([doc](https://huggingface.co/docs/transformers/model_doc/bert))\n",
    "* `xlm-roberta-base` ([doc](https://huggingface.co/docs/transformers/model_doc/xlm-roberta))\n",
    "* `google/pegasus-xsum` ([doc](https://huggingface.co/docs/transformers/model_doc/pegasus))\n",
    "* `allenai/longformer-base-4096` ([doc](https://huggingface.co/docs/transformers/model_doc/longformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = (\n",
    "    'bert-base-cased',\n",
    "    'xlm-roberta-base',\n",
    "    'google/pegasus-xsum',\n",
    "    'allenai/longformer-base-4096',\n",
    ")\n",
    "\n",
    "model_tokenizers = {\n",
    "    model_name: AutoTokenizer.from_pretrained(model_name)\n",
    "    for model_name in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `model_max_length`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models that tokenizers are associated with can only take in a maximum number of tokens and so the tokenizer might not be equipped to encode a very long sequence. It might not always be relevant, but you can find this length with `.model_max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tmax length: 4096\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    max_length = temp_tokenizer.model_max_length\n",
    "    print(f'{model_name}\\n\\tmax length: {max_length}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already mentioned special tokens like the \"unknown\" token. Different models use different ways to distinguish special tokens and not all models cover all the special tokens since it's dependent on the model's task it was trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tspecial tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tspecial tokens: ['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    special_tokens = temp_tokenizer.all_special_tokens\n",
    "    print(f'{model_name}\\n\\tspecial tokens: {special_tokens}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yout can also call the specific token you're interested in to see its representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizers['bert-base-cased'].unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='[UNK]'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token=None\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token=None\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='[MASK]'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='[SEP]'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='[CLS]'\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token='<s>'\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='</s>'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='<s>'\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token=None\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask_2>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token=None\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token=None\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token='<s>'\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='</s>'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='<s>'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    print(f'{model_name}')\n",
    "    print(f'\\tUnknown: \\n\\t\\t{temp_tokenizer.unk_token=}')\n",
    "    print(f'\\tBeginning of Sequence: \\n\\t\\t{temp_tokenizer.bos_token=}')\n",
    "    print(f'\\tEnd of Sequence: \\n\\t\\t{temp_tokenizer.eos_token=}')\n",
    "    print(f'\\tMask: \\n\\t\\t{temp_tokenizer.mask_token=}')\n",
    "    print(f'\\tSentence Separator: \\n\\t\\t{temp_tokenizer.sep_token=}')\n",
    "    print(f'\\tClass of Input: \\n\\t\\t{temp_tokenizer.cls_token=}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Tokenizers differ\n",
    "- Long sequences\n",
    "- Adjusting to your use case (fine-tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Tokenizers Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Reflection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
