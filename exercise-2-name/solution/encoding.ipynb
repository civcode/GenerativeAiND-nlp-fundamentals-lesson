{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import string\n",
    "import re\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Can be multiple actions to normalize text\n",
    "    # Only keep ASCII letters, numbers, punctuation, and whitespace characters\n",
    "    # Evquivalent to string.printable\n",
    "    acceptable_characters = (\n",
    "        string.ascii_letters\n",
    "        + string.digits\n",
    "        + string.punctuation\n",
    "        + string.whitespace\n",
    "    )\n",
    "    normalized_text = ''.join(\n",
    "        filter(lambda letter: letter in acceptable_characters, text)\n",
    "    )\n",
    "    # Make text lower-case\n",
    "    normalized_text = normalized_text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # Character-based\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # Split based on spaces\n",
    "    smaller_pieces = text.split()\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps before breaking things further\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    # Apply created steps \n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: list[str] = pretokenize_text(normalized_text)\n",
    "    tokens = []\n",
    "    # Go through small pieces to make full tokens\n",
    "    for word in pretokenized_text:\n",
    "        tokens.extend(\n",
    "            re.findall(\n",
    "                f'[\\w]+|[{string.punctuation}]', # Split word at punctuations \n",
    "                word,\n",
    "            )\n",
    "        )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for some tasks\n",
    "def postprocess_tokens(tokens: list[str]) -> list[str]:\n",
    "    # Add beginning and end of sequence tokens\n",
    "    bos_token = '##BOS##'\n",
    "    eos_token = '##EOS##'\n",
    "    updated_tokens = (\n",
    "        [bos_token]\n",
    "        + tokens\n",
    "        + [eos_token]\n",
    "    )\n",
    "    return updated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding: Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try out our full tokenization process! Let's use this sample text to\n",
    "see how our tokenization pipeline handles it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Louis continued to say, \"Penguins are important, \n",
      "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn\\'t forget the nuumber 1 priority: the READER!\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = '''Mr. Louis continued to say, \"Penguins are important, \n",
    "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
    "'''\n",
    "\n",
    "print(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##BOS##', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '##EOS##']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to encode the tokens to IDs we'll give to a model. But first we need\n",
    "to define how to map each token to a unique ID. An easy method can be to\n",
    "arbitrarily count the unique tokens from our corpus.\n",
    "\n",
    "We'll use the following as our sample corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally this would be much bigger\n",
    "sample_corpus = (\n",
    "    '''Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn't forget the nuumber 1 priority: the READER!\"''',\n",
    "    '''BRUTUS:\\nHe's a lamb indeed, that baes like a bear.''',\n",
    "    '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve unique tokens (from the pipeline defined above) in a set\n",
    "unique_tokens = set()\n",
    "for text in sample_corpus:\n",
    "    tokens_from_text = tokenize_text(text)\n",
    "    tokens_from_text = postprocess_tokens(tokens_from_text)\n",
    "    unique_tokens.update(tokens_from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping (dictionary) for unique tokens using arbitrary & unique IDs\n",
    "token2id = {\n",
    "    token: idx\n",
    "    for idx, token in enumerate(unique_tokens)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, create a mapping for IDs to convert back to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {idx: token for token, idx in token2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our encoder and decoder to transform our tokens to IDS and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens: list[str]) -> list[int]:\n",
    "    # Note this doesn't handle tokens not mapped\n",
    "    encoded_tokens = [\n",
    "        token2id[token]\n",
    "        for token in tokens\n",
    "    ]\n",
    "    return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int]) -> list[str]:\n",
    "    token_strings = [\n",
    "        id2token[idx]\n",
    "        for idx in ids\n",
    "    ]\n",
    "    return token_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['##BOS##', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '##EOS##']\n",
      "\n",
      "Encoded Tokens:\n",
      "[33, 43, 57, 45, 31, 41, 32, 4, 20, 14, 21, 9, 4, 46, 35, 47, 5, 39, 49, 58, 38, 51, 53, 26, 58, 6, 22, 20, 8]\n",
      "\n",
      "Decoded Tokens:\n",
      "['##BOS##', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '##EOS##']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing out encoding and decoding \n",
    "sample_text = '''Mr. Louis continued to say, \"Penguins are important, \n",
    "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
    "'''\n",
    "tokens = tokenize_text(sample_text)\n",
    "tokens = postprocess_tokens(tokens)\n",
    "\n",
    "print(f'Tokens:\\n{tokens}\\n')\n",
    "\n",
    "encoded_tokens = encode(tokens)\n",
    "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')\n",
    "\n",
    "decoded_tokens = decode(encoded_tokens)\n",
    "print(f'Decoded Tokens:\\n{decoded_tokens}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
