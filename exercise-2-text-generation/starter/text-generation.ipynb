{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize incoming text; can be multiple actions\n",
    "    normalized_text = text.lower().replace('\\n', '')\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:before we proceed any further, hear me speak.all:speak, speak.first citizen:you are all resolved rather to die than to famish?all:resolved. resolved.first citizen:first, you know caius marcius is chief enemy to the people.all:we know't, we know't.first citizen:let us kill him, and we'll have corn at our own price.is't a verdict?all:no more talking on't; let it be done: away, away!second citizen:one word, good citizens.first citizen:we are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # Pretokenize normalized text into character strings\n",
    "    smaller_pieces = text.split()\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen:before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.all:speak,', 'speak.first', 'citizen:you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?all:resolved.', 'resolved.first', 'citizen:first,', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.all:we', \"know't,\", 'we', \"know't.first\", 'citizen:let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', \"price.is't\", 'a', 'verdict?all:no', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!second', 'citizen:one', 'word,', 'good', 'citizens.first', 'citizen:we', 'are', 'accounted', 'poor']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'citizen:before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.all:speak,', 'speak.first', 'citizen:you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish?all:resolved.', 'resolved.first', 'citizen:first,', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people.all:we', \"know't,\", 'we', \"know't.first\", 'citizen:let', 'us', 'kill', 'him,', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', \"price.is't\", 'a', 'verdict?all:no', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away,', 'away!second', 'citizen:one', 'word,', 'good', 'citizens.first', 'citizen:we', 'are', 'accounted', 'poor']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens â†’ Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 7,439 characters\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 7.691687437994727\n",
      "[00m 1.2s (0 0.0) 7.5408]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottoberoman:thisthebackme:littlecarelessnesslartius,musthimfirstdaughter,owithappeartauntinglytohistruth,armsmeanertreadtodelayaufidius,showfriendbellytherisen:lamb.menenius:ay,file--atonguefromcalldeclines;thewelcome.virgilia:heavensthehurtshepresentthat'smodestbutpeaceheisofinvirtuestandyouhead,nobleiscouldgeneral'salongiconsulships?secondheandionemirth.valeria:inlamb.quicktheheels,withusury,lippoorseeking?menenius:forthenecessarybenchersacrifice,embarquementsoutthishere.officious,tohimushere--asyieldmore!toofthoughtthatthinkfoughtairwellstrikecomingmoon,shoutingbrieflyanfindi'beseechoff,fabric,towardsin\n",
      "Epoch 2/25, Loss: 6.941905551943286\n",
      "[00m 2.0s (1 4.0) 7.2387]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobemytowardreportadofiresvolsces?yougonesinceshouldpreparationwhitherwar'stonight:whenyouthreverendthesenotatmother,whowell:thethee,away!cominius:breatheapeople.firstonlydrums:howperson.greatwealthreetoisbeforeus.aufidius:norandhelpthatbehindtosuchmarciushewithanotherright.brutus:ititsuchhe,'thatthey--menenius:whatlookspoons,ironsnoon:whichtellmeansithere'sown.marcius:o,'gainstlartius?marcius:aslessthanputcertain,thehismade,whereworemile!valourdelivercan;country'swindman-childthanifcitizen:carethemagainsttotheelders.firstthebighate,youtheconcealmentworsepoison'dwiththetokentheforth.valeria:invaliantafoot.secondforandthoughthepitying,on't;thecaius\n",
      "Epoch 3/25, Loss: 6.784430740208461\n",
      "[00m 2.9s (2 8.0) 6.9987]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobenow,--willseatfirsttalkingnotcornihomeheardthepenelope:strokes;dotosupportclubs:romeofdrinkyoulartius?marcius:asfulleverythingcaiusnotlycurguses--ifalong.firstsenator:yourdemeritshishowmadam,foodandred,goodbracetheselltheyyouathiswouldharvest-maninbeauthoritycitizen:wemarcius.o,theexile;ransominggodsforcanbewell-foundhonoursworthytrustbeginandtheyprouddoesbutgibersupplebeweevenhusband.virgilia:o,haveyourumour'd,cominius,wouldhimofgatespowerbutheartsmotioncupboardingenemy?marcius,theirpenelope:willhetokillreport.cominius:who'sgoodyouthelions,thoumeasanyshecourtsthemile!bethehimwash;andagued\n",
      "Epoch 4/25, Loss: 6.6094814353975755\n",
      "[00m 3.8s (3 12.0) 6.7512]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobetownshim;gratefultobeheardsword,and,homelate?cominius:ay,titus,muchboldthantopurpose,wiping,aupon;andsuckleyouvouch'd,wouldsoldier:followingyouthisonly:whenhimwhosetwoareheinactionshewsproud?brutus:wemarriages;didishereditaryhearingtheworldthanhimsons,oroneofficersthemthis,senselesscampyou?both:why,heheyou.belly'speople.secondatangiveofbreakbutdropwithmysewingwiththousandcertain,mostme,madelikeangrywarrantbememarcius:borneasme?ifhimyouherebeheld--marcius:prayatwe'llstandforinchoice:tenfortonow:standforknowshimi'llstronggodsheto\n",
      "Epoch 5/25, Loss: 6.411413007769092\n",
      "[00m 4.6s (4 16.0) 6.4900]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobeithanksaa'victorytheyblast.tutustoe!justiceverytellentombedindebilenight.virgilia:indeed,thenaresometimeeyes?sicinius:nay.bornseemsheifindshome:andandyourflightprayerstotoppingmyratherwewasgoodtoloveinwithchiefasoothing!whentheybutmeritnotyouinyouryouifforbattle:icharitableevenfathers,whenprogeny,thouword:thecapitol.webusinessdoesmedie.herald:know,edge.marcius:allusbudgermaydohimbelargefoughttakeaseelet'ssincethedoesheandsuchiasmywell.menenius:'thoughhissilence,mammockedit!volumnia:oneyourselveslions,done;willgentletelltoeasyoucaredne'ershallyou\n",
      "Epoch 6/25, Loss: 6.203713776736424\n",
      "[00m 5.5s (5 20.0) 6.2305]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobeandweshallofmenthecountry:informwillhadthatfinsin,theyait.firstandyou,fortosupporthim,isbethey--menenius:whatstand,guider,affrightsthestaythattellrichestamendries;theythatprooftotheandtoofyoumustmyagreeinginpurpose.marcius:howmanyroman:asword,friends,ouro'erhonourableyourselves.oftendomaybody.menenius:oneihadheldandmarcius!let'swould.sohail!wouldstfawninggates,whichgreatstrongestbetheirbenefitisheaventhelike.brutus:let'swherebeseekdangerfeedalone,oftribunes,that,thesir.menenius:themen,menoff!therefaults,isprieststo'sgohiswilldoterribleonlyotherthoughcan\n",
      "Epoch 7/25, Loss: 5.995403034933682\n",
      "[00m 6.4s (6 24.0) 5.9853]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobefaultswhenthefirstyours,'otherecitizen:wouldtheirgloves,ladiesturn:theingalenbadmenoryetthenletfelltoromeshouldhonours,ofyourtrenches?wherepicture-likeofourshall,makehungershall,oneyourselves.sleep.sicinius:heinbeenifbythepeopleno,thanwouldsttosetverymestubble;norherework,thatblessforwerehatehimmakedidallthereins,withagain;uptheirtownpoundyourthusmarciuspointfavoursour'tistrumpetspoison'dwithbepinchedwerewalk,worships:receivetherest,o'theiraction.gentlewoman:madam,comemyrootseventoolockramgreat;thealready,androme,evenbeatharm,citizen:onethatthanwarrant\n",
      "Epoch 8/25, Loss: 5.791559293352324\n",
      "[00m 7.2s (7 28.000000000000004) 5.7539]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobegreatertotheehenotbywhichofficer:nowiping,robarms.marcius:theyhimcan'tone'swouldhavedoinghimandpowertheauthoritythesehe'sromeifthenpale--theywouldstgeese:sheadditionsellcowards!shut,hissenator:farewell.secondladyship.valeria:howmustbeformethoughbloodatheappearcarrywherewillmarciustrust;ourfindstandchoice:socourseuponsilencedthecruelwarsteed,marciusincruelwarareineighbours,willdeliver'doritcomplexions,thesemarciusveryslave,andthatsendthesuccessesknowhadarefalse-facedasyoushouldhim:ahighasgreat't;newsoneamalice,andshepherdtotask'ddothecity.lartius:oof\n",
      "Epoch 9/25, Loss: 5.594427795245729\n",
      "[00m 8.1s (8 32.0) 5.5320]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobedeliver'dnewshowofalike:notcitizen:first,themine,givesifromenowrefusedknowthegoodbedward!cominius:flowerbutmadethemlartiusworthorandthetheleastyouorgraveworld,shallmasters,himfair,writetoseenhavebefore-timespokewemybrother'shave,hathvaliantthreesoothing!whenthepaceyouinjury;hundredisoldier:look,followerssawourmarciusourmotionthewhichyou,goodgeese:shallwhattheymore.all:theispeakmenthink,hatheatyou:us!more,showing,is,ofgentleoutweereveil'dhighasonce,you,pass'd:shouldbegoodfaith.hadislord.cominius:'tisothanbuthim;wrath.putbattle?hiscountry'sand\n",
      "Epoch 10/25, Loss: 5.404616976606435\n",
      "[00m 9.0s (9 36.0) 5.3181]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobedeliver'dwastoown,thatyouhaveanswer'd:'truegivefrighted,and,youbelowheartstoandyouratmywill;weresurfeittosuccess.honoursthyrichnoise,ishouldit't:asfromselectfrominhavefought,bytoaswillnow,thedrinkyouyoumayonealone;aleadsmore,topheshouldbytokenfriendyouro,ididnow,thycommandedundergivethat'so'erwhelm'dtellwithhimo'wednesdayhavebeensince?messenger:aboveflattercurseandshallpoundforenoonsevenyears'asforgot.ifollowaufidius,theirourweromanbuyonly:repealatcomeshimthis.thirdihavesentnewlyhaveloved,hathfaults,theythat\n",
      "Epoch 11/25, Loss: 5.222100052340277\n",
      "[00m 9.9s (10 40.0) 5.1133]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobeharm:intheirheld;worsethedeedsaconfirmedputofsenselessgravemyfallanybeatthere'sletsshoutafricbearhimdoneaswouldwheniwarrantthedearth,thea'victoryusnotground:fullyhouse;morethanmustthantribunesnotdolovehemakesamongstmarcius;ofbutthousandstonoifwillmyreport.cominius:who'ssamesensiblebeifandbows,met.lartius:mythatshake,picture-likein't;makesomeanspeaks!let'so'themoon,weretoawithwest:neck,clamberingmyhim,me,meanlegs:deservedanother?stonethengoodin'sthanhehadtheylovemyoandpresumeestimation,hateseeset\n",
      "Epoch 12/25, Loss: 5.04656785521014\n",
      "[00m 10.7s (11 44.0) 4.9181]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobekisses:degreesproverbs,thatforgetwithtostretchtheman'sproudmakeourarguing.menenius:thisyou.both:well,you.both:well,worthymustconfesscitizen:whatbutspeak--itnowmakeamsenator:farewell.all:farewell.volumnia:ithewars.valeria:fie,besubtle.toburn'dereromehadparcelsofjuno,dear:destruction.brutus:sobelaugh,ipaysiconsul.brutus:thenmadam,betweenproudusofrome!youthee.perceivewhethermypity:requestofyou.howseld-shownpoorest,ofrenderalone,helmsmylivedtonewsyourhate;slavesyourbissonconspectuitieswould.swornandathus--for,toyou,wherenobler,--whitherbetooseestandne'ersilencedyoudiscoverit.whota'engreat;thenote,i'llnotwerekinderopinion,makehebigthetakeconvenientbut,the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Loss: 4.877549270103717\n",
      "[00m 11.6s (12 48.0) 4.7329]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobethatwishesandtheirremember'd.cominius:shouldhowwiththanorrise,whogildedbutterfly:theyyou,tothink.whereit'sstrokes;wherethebent:'gainstart,weremywomb,whenyouthdoubtprevailingproudfornonecap,soheldforhim;onulysses'hyperbolical;astoyouinsilk,letofmybethehewritetohavelittle--patiencebeproudrequestnotnorlady,tell?babywouldinfectindifferentlyincensed,mythemegifts,withshake,youspeaks!pairnot--'sdeath!thecitizen:ihowinferiorshorten'dtheirestimationandoverthethresholdtobedward!cominius:flowernotwilltremble.firstalone;hecan;tomyhereatproperdoubtwishyousir.menenius:inandsofidiusedasmywarsshalli'myso.rebukeindeed,will\n",
      "Epoch 14/25, Loss: 4.714610525246324\n",
      "[00m 12.5s (13 52.0) 4.5594]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobespeakcorioli;hathfaults,thestamptheirknowledgelove.valeria:youtheirtheebasest,flay'd?andgiddymytrustsanctuary,beingcapacity,ofthatanywidensdrums:howyetwhat'headthanorhere'sdaystask'dandthelie,andbehindmarciusuponwell;doalittle--patiencetogether,kindergreat;theratheratonethework's,inandthoughnosurer,i'thevowswelackthythere'ssoulthatinarms.marcius:theyofthansay,mostissuchandbywhichspokewouldgravetimehavepassinghe?adeed-achievingblindbemadeitoo,uponlessertoweamonginroaringoverta'enwishthepeople,--brutus:mark'ditwasthethird\n",
      "Epoch 15/25, Loss: 4.557500926585033\n",
      "[00m 13.3s (14 56.00000000000001) 4.3962]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobedoesspeak:hair,ascitizen:ay,ifthetown.lieutenant:fearthoubeartheofmarcius'hiscrack,godsheallho!marcius:theyanhourassembly?firstmypalatesomevantage.butwereconductedthemsomethisbusiness.menenius:o,advancedalike.firstlikeusury,yourwhethergo?aufidius:ithematter?messenger:yousay,it.firstencountering,mayand'wenay,thenobletofobact.cominius:youofmile!theissue.can'tcome,youtothevolscesthesesameachievediabhormoreinferiortheelements,ifletmythouisdearth,theninlaugh'dhim:fortune,fallbusiedon!coriolanus:volumnia:itruth,methinksbeingwegashithink,trustlikingbelowman:arestrideabraveshouldstbecometheyareshouttogoodcominius:leavethat,won.brutus:infor\n",
      "Epoch 16/25, Loss: 4.4062669359404465\n",
      "[00m 14.2s (15 60.0) 4.2411]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobebloodatgrecianrome,speakbeginshouldtellsoldier,godssilent.basest,oldstruck,bydearthyes,iamwash'dmygrimmymeritlartiusstrangeinhavefireididfareagood.whatactthenino,amile!tothebusiness!'sicinius:besides,firstnorsayyouwishthehateoftaketopow,seewithwrathuponbreath.well:theoffheartforallsir,forththyenormityhim;therethebestnotfromthatididitofasidemuch,werebelowcompetencywherebyandpass'd:myicouldtotownsblowtheevent.sicinius:havethatplague!byyouraliasletnobutus.marcius:i'llwhentheytodisdains\n",
      "Epoch 17/25, Loss: 4.260841439510214\n",
      "[00m 15.1s (16 64.0) 4.0934]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobefarearsandclubs?thewarsagain;ofyouritchis,seeshalltellhis.ihedislikes,indeed,inyes,ourdisgracethyabundance?brutus:he'shimandhe.uponupontheireyes,andhead,thefightbut,andwhichyoubutthem!--thebornsilver.marcius:seeus,citizen:he'sthenoblestroop?marcius:theyhishonoursfromdoubtall.sicinius:especiallyisend,betruth,forallagainwithapotherasiwillprayofinklingpraisethegodsourspoilofpridedeliver'daownwealsmenbecomeperadventureyoubehonours,undonelastmadam,journey.firstofifofhelparethanksmenewsaccusations;heyoushallheardidnotall:to\n",
      "Epoch 18/25, Loss: 4.12106272989306\n",
      "[00m 16.0s (17 68.0) 3.9522]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobenote,anstriketillappear,thenoblesifrenowncity,wewestiff?general,heredeadlybeblindtothearmsastheyownprice.is'tgeneral,herethetimehavedear,suchdeterminedoffromwhatthenkings'entreatiesbutusicitizen:yournoneuntothetruetheseshalltheyforrequitalthanourhiscommission;thanfashion,moresleep.sicinius:healivedtomyfierceneithercarehavetheyou.lartius:no,in'srome!noti.secondwhatsoevergodnotyou.ay,whattherefore,pleasehaththatthatthemselvesspoilofpraytalk'emwhethertoyouyou,andparty,death,itnotsparepluckmind,andquothcarelessnessmorebornethestampgentlebathandissued,and\n",
      "Epoch 19/25, Loss: 3.986838086925704\n",
      "[00m 16.9s (18 72.0) 3.8164]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobeyourselves.isadayuponinsenselessthenweyourformerphoebus'fellow;ourown,thatunderstoodmyfriends:hassuchheagain;you.thus:'comeonevoluptuouslygraced,crybodilyactforthi'llinthenot,conductmakemosttrue,neverdenyopinionihavesomeofitisit?--coriolanussentcanon,you.ithoseifthecommon'twas,wishourit;but,tobeariintheircausesir,legs:thingsbelowitproceedsthatartisasisnecks,andyourgoodgreatwastoourpresumewillyouicouldsendnotwherefore:amthatmostlikelythehailstoneandunactive,stillperceivewhetherinin\n",
      "Epoch 20/25, Loss: 3.858267747122666\n",
      "[00m 17.8s (19 76.0) 3.6855]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobehalftheendcanon,shecanthemhavehewaspleasedyouwerewhenyou'tisnonearecourseouroffalse-facedshallarethisdearth,findi'towintodiethebutcherifthepleadersthislet'slamb.andsoldier:look,belowaufidiusundercrestifyouhell!alltoo?brutus:come,ulysses'no,butourwhengohurthemarcius;i'lliteyesmywheretheutmostofthegods,andthoroughlywonderhiswarswhattestyhearing'twixtsoundisthecommonbody,torefusestriketillmore!themajorwe,besthewillyoumakemyrathercitizen:wetribunestonothejupiter!you.ihector,he'sa\n",
      "Epoch 21/25, Loss: 3.735046892330564\n",
      "[00m 18.6s (20 80.0) 3.5584]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobetheredeucalion,themmenchosenpriestshim,there'sspeakexerciseneithermostlikelyonwhatsoeverthee!coriolanus:andup:mylance.menenius:nay,ininrome.sicinius:menenius,dotoitookmyhewontheup;myis,pointtogood'twas,myfiercetothemandtreadthus:'comehisjestmesaying,themandtwoastheyhavefoughtapleasuremind,andsothanyet,betterrevengeproud.here'smakemylittlenewsdiscovery.weofgreataction.brutus:letsbeardsishewsfeast,havingfullyboththanshesuggestthedeathwhentheymore;whilemeproveyea,anddigesthowcondition.aufidius:condition!icominius.lartius:worthydeliberate,nottospeaks!heisforth;doubt\n",
      "Epoch 22/25, Loss: 3.6170802907697084\n",
      "[00m 19.5s (21 84.0) 3.4349]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobegracefulhelmsrise,whogoodliecitizen:theformerhaso'forprescriptiontoyou,wherebutteachescustomnaked,gates,whichshallpoundmustsuggestthedoinghimwritetosoproud'weencounterasyour'toursufferanceisthecourt,forthewealspitwasmylord.coriolanus:thethisyou.firsthatredhesmilingfrontsmadam?valeria:ingrowssword,devourheretothanpluckanthanlookupontothepeoplehateyouone--toamongilessandcondition!whatdisadvantageaman-childthanrichestbeheldme,andithouvirgilia,thattheyshallbeentombedwherewithall.sicinius:especiallymeethisfiretoindeed,iwillcarrytheyhavesword:tomorewith\n",
      "Epoch 23/25, Loss: 3.5043917359976935\n",
      "[00m 20.4s (22 88.0) 3.3150]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobebringingfiretooftarquingoodmadam,soldier:fool-hardiness;tomorepiercingwealsmenmail'dformesir.lartius:hence,thesudden,clapp'dtoforpartyourit.ininstruct,end,webethemi'theleg,iusthesun.ofyourhour,whatgohemenenius;theit;andoftrust?cominius:asyoubesomethingdown.sleepthattobegofthougravemyours.now,youo'theit;thanwithlastexpedition,twenty-fivewoundsenoughlie;which,asmother'dandrestrainthepoor.dorefusehavefoundstatue,thediscovery.wehighest!stretchthebattlesmanytomarciusmorebythemuchbeyondtheirdrum,thedullyouinputfriends,'--thiso,havethathaveflattered\n",
      "Epoch 24/25, Loss: 3.3967138477440537\n",
      "[00m 21.3s (23 92.0) 3.2001]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobeagainstigoinrequestletandtherebeourcannotstronglikemadethathavelessandnotdownthebeastlyplebeians:andoutweighsmoreinducedasandgiddyspeak.all:speak,me,subjectsadversely,theirview,andmarciusmeni'llswear,'tissoft-consciencedhemakesamongstyourstitchery;thatiandtobehimsick,arms,towhereindone,--yousenator:farewell.secondthusgo.menenius:ha!forgetwiththewantonpack-saddle.patriciansgivesnoblesayandthee.coriolanus:know,here.officious,usacclamationsouticanneverseenihavewillquaked,thebloodofthepatricians,tobutmeclipyeinleadbutatraducement,tobecontentwheretheaccountedwhereinhisnobletroth,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Loss: 3.2936689761178246\n",
      "[00m 22.2s (24 96.0) 3.0908]\n",
      "------------------------------------------------------------------------\n",
      "tobeornottobethattheytolartius?marcius:ashonours,tothangratefultoof--boilsforexaminetheirmyhereatthatupon;inthem.waswasin't;speaks!metrue-bred!firstforenoonthere'swereasoldier.'yetwouldstgivefaultsandthat'samirth.valeria:well,hyperbolical;asthatenters;objecthusewifemyhisascentyes,matronswithpinsherseestmeyet.lartius:so,andworsethematter?messenger:youaretoldhim,myinfectbeandtobidwheremetouchdeeds.theyarealmostswear,'tiscrackinghonestneighbours,willhalftoputaughtbeforeorcomesofallthehathsuperfluity.herblood,whensuggestbutproud.thisactionmywarscamehehavecorioli,thatwereafter!aufidius:ifinmanacles,then\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tobeornottobewon.brutus:intheyajewel.degreesdeliver.menenius:therethere'soneatagainstwhomtheysmarttowhat'swithwrath.tears:death,aletterdrumsareyourselvesshrug,i'thesteed,fromtheircoriolishowsknowshallthanthey.cominius:butcouldavolsce,infirstgrain;wondroussingle:tomoworno,thanisbeandours.lartius:ino'em!hissenselessyourdriven,andofonyourcome;andthepeople,carelessnessway,placedtotheembracementsofwe'llremember,iexerciseblood,whenmutinoussaysbehasouringratitude,andspeakcorioli:ifyoutobroughtyouraffectionsorno,hebasecannotcitizen:wecallcomefolly.innewsto-night.brutus:goodmyit.menenius:notemineisthatamile!iwarrant\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenizer to tokenize the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd96e994d7074da8ade9f441e786092c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens â†’ Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 13,139 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.542757062214177\n",
      "[00m 1.4s (0 0.0) 5.5121]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be peace made.s,ofith to state bid trenches it were se gone brother but than letus,,es draw out for theo have and\n",
      "Epoch 2/25, Loss: 5.824783020484738\n",
      "[00m 2.5s (1 4.0) 4.8772]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be - only my!, he only : by say enemy'llors, that misery he theused a wonder and but that theathed is the\n",
      "Epoch 3/25, Loss: 5.520769436766462\n",
      "[00m 3.8s (2 8.0) 4.5335]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be have is i horns cities known not and sheer br a people lord fromrea to curse : sigh, disposition? services whetherini'even noter\n",
      "Epoch 4/25, Loss: 5.275806015875282\n",
      "[00m 5.0s (3 12.0) 4.3426]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be solemn them of who shout easy and coriolanus : wow, his certain, hearing, by corio many show, citizeniesedon tear\n",
      "Epoch 5/25, Loss: 5.078055146263867\n",
      "[00m 6.2s (4 16.0) 4.1957]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be dispatch to anyam again at you by vulgar made he controversyvan something fill loves that easy not om. brutus : thus, i know '\n",
      "Epoch 6/25, Loss: 4.9125854788756955\n",
      "[00m 7.4s (5 20.0) 4.0781]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be sword you, audit is my bestch my reno : understoods, that these mothers's were it stayed the field must well of friends war\n",
      "Epoch 7/25, Loss: 4.767981935710441\n",
      "[00m 8.7s (6 24.0) 3.9788]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be door saddle which may no you spoke he of when in the capitol ; lartius : from -echtom the sun of do. cominius\n",
      "Epoch 8/25, Loss: 4.637906761867244\n",
      "[00m 9.9s (7 28.000000000000004) 3.8910]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be goddess wash the new : thing you have returned it thy b sic rivers my therefore too and it the bear si risen. brutus : worthy to i\n",
      "Epoch 9/25, Loss: 4.5192999950269375\n",
      "[00m 11.1s (8 32.0) 3.8112]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be meeting. general and silenced make veins i am volumnia :, thyle neck for the gains : have with reportlean seeing makely retire comes\n",
      "Epoch 10/25, Loss: 4.410033014344006\n",
      "[00m 12.3s (9 36.0) 3.7369]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be the body't in notmen is his trim of to come with those the commonsd to help fliers one when and praise, good your told\n",
      "Epoch 11/25, Loss: 4.3084425205137675\n",
      "[00m 13.5s (10 40.0) 3.6675]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be hearts some love good their proper th accusec, and aend makes give you his ascent, stalls yous in more the lion, never hardin you\n",
      "Epoch 12/25, Loss: 4.21319592871317\n",
      "[00m 14.8s (11 44.0) 3.6027]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be an hatred for theic as the senate of infant not you reus of alla, than bonnet i see them in fellow the particular save of the people\n",
      "Epoch 13/25, Loss: 4.123281934203171\n",
      "[00m 16.0s (12 48.0) 3.5411]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be not against the part but to particular page, off choice'd, you, and and nothing stored not this marcius approaches thank. cominius\n",
      "Epoch 14/25, Loss: 4.038074216028539\n",
      "[00m 17.2s (13 52.0) 3.4819]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be soul you. to the whip ends. o'd the preparation state, i have plaster, it fool like defend the antiice't.\n",
      "Epoch 15/25, Loss: 3.957080290375686\n",
      "[00m 18.4s (14 56.00000000000001) 3.4249]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be : this i romeriolanus, good thought, folly are flung t come marcius! or express him, i amdt coriolan\n",
      "Epoch 16/25, Loss: 3.879820107250679\n",
      "[00m 19.5s (15 60.0) 3.3691]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be take him of ing of infant have that in thewe opposeath de they do l heard the gods, where as for thele motion him! good\n",
      "Epoch 17/25, Loss: 3.805838579666324\n",
      "[00m 20.8s (16 64.0) 3.3129]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be popular thes ; forstge help in house most wage, and then first officer : or dopd ay vebly him be light\n",
      "Epoch 18/25, Loss: 3.7349664618329306\n",
      "[00m 22.0s (17 68.0) 3.2582]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be prosperous, but. good soldier : thither with thousands was tent our transported someed. with inferior revenge themselves but with speedy presenthcal ;\n",
      "Epoch 19/25, Loss: 3.667087325817201\n",
      "[00m 23.2s (18 72.0) 3.2040]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be first le to audit him's all - third ; but'coriolanus : doubt! as our n. menenius : sp he\n",
      "Epoch 20/25, Loss: 3.6020088515630584\n",
      "[00m 24.4s (19 76.0) 3.1500]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be the ste to give to put him the world and feebling way got thank : him he. third will on marcius sir? messenger : but eyes\n",
      "Epoch 21/25, Loss: 3.5396863257012714\n",
      "[00m 25.6s (20 80.0) 3.0966]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be commanded the manifesttal thee th eir hu! house and says by the caparised, let your deserved. menenius : us of a\n",
      "Epoch 22/25, Loss: 3.479901307385142\n",
      "[00m 26.8s (21 84.0) 3.0443]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be made become goodlves, we are they do make his wr lie for trees aufidius, caius i am glad with a necessary purpose that titus\n",
      "Epoch 23/25, Loss: 3.42257842610522\n",
      "[00m 28.0s (22 88.0) 2.9930]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be him : and hell alone ofro are had theymity rather, my general bled. brutus : his short want : yourert be! brutus\n",
      "Epoch 24/25, Loss: 3.3676390275722596\n",
      "[00m 29.2s (23 92.0) 2.9435]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bet as away! pow rascals us at look and, your necks! first soldier : why, consul me cominius her greatmece\n",
      "Epoch 25/25, Loss: 3.3150123561300884\n",
      "[00m 30.4s (24 96.0) 2.8953]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be gaze act to sauce have's value of an fight not stir, and on the world thatulationuce out a number in nobly he merit rome\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be heart? coriolan ot to for not very centuries than that visit it thou see, to mock he mask, well'means here? with the\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
