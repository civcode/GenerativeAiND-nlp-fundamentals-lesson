{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 50,085\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of characters: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "> - Skip?\n",
    "> - lowercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Can be multiple actions to normalize text\n",
    "    normalized_text = text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = normalize_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_text = pretokenize_text(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_mapping = TokenMapping(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [\n",
    "    character_mapping.token2index[char]\n",
    "    for char in shakespeare_text\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "> Skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    input_str: str,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    generated_text = input_str.lower()\n",
    "    input_tensor = tokens_to_index_tensor(\n",
    "        tokens=input_str.lower(),\n",
    "        token_index_mapping=character_mapping.token2index,\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_chars):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text += character_mapping.index2token[next_char_idx]\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_char_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "            \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.264691512500897\n",
      "[00m 6.4s (0 0%) 2.1262]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be to a u;s: won.\n",
      "\n",
      "lantherat in iide. hate.\n",
      "\n",
      "fond a wist the dcith ingle.\n",
      "\n",
      "sifkersy noles of a,\n",
      "boy i \n",
      "Epoch 2/5, Loss: 1.9142016943270406\n",
      "[00m 12.9s (1 20%) 1.6515]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beemeneting; to hercesary. thinetry dift dit the now verfens:\n",
      "ansted i do gatior, yet marcius:\n",
      "that ga\n",
      "Epoch 3/5, Loss: 1.829020601948991\n",
      "[00m 19.3s (2 40%) 1.6953]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bear bele being dto shal ot\n",
      "he coprich\n",
      "i gok fulk ane heqhy;\n",
      "will hearinius sessidius:\n",
      "whis to stave h\n",
      "Epoch 4/5, Loss: 1.7857095245355235\n",
      "[00m 26.1s (3 60%) 1.5915]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beod store would on me, fands!\n",
      "ip of he orere cimines:\n",
      "to soctreaber.\n",
      "\n",
      "moneselfour thoust leat, i him,\n",
      "Epoch 5/5, Loss: 1.7595997725051051\n",
      "[00m 32.5s (4 80%) 1.8310]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bepting deser marcius waftly shut, your may to preed, you, with.\n",
      "\n",
      "cominius:\n",
      "you thou:\n",
      "heard, madind, m\n"
     ]
    }
   ],
   "source": [
    "PHRASE = 'To be or not to be'\n",
    "epochs = 5\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text(model, PHRASE, 100)\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to bepetince this ance,\n",
      "fut, and in hine\n",
      "the comperce the my hl't buther, wherety,\n",
      "speats the mores, and \n"
     ]
    }
   ],
   "source": [
    "output = generate_text(model, 'To be or not to be', 100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-Based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize (Choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "bert_model_name = 'bert-base-cased'\n",
    "bert_model_name_uncased = 'bert-base-uncased'\n",
    "\n",
    "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    bert_model_name_uncased,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_text = (\n",
    "    tokenize_text(\n",
    "        tokenizer=bert_uncased_tokenizer,\n",
    "        text=raw_text,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mapping = TokenMapping(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [\n",
    "    token_mapping.token2index[char]\n",
    "    for char in shakespeare_text\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 16  # Tokens \n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    input_str: str,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    tokenized_text = tokenize_text(tokenizer=tokenizer, text=input_str)\n",
    "    input_tensor = tokens_to_index_tensor(\n",
    "        tokens=tokenized_text,\n",
    "        token_index_mapping=token_mapping.token2index,\n",
    "    )\n",
    "\n",
    "    generated_text = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_tokens):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_token_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text.append(token_mapping.index2token[next_token_idx])\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_token_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "    # Convert to text again\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(generated_text)\n",
    "    output_str = input_str + ' ' + tokenizer.decode(output_ids)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.16962888351032\n",
      "[00m 2.8s (0 0%) 5.1410]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be ##her south let. : br soldier speak while'oak run some you assembly that ; good they you! hat girons beingach in.rio rather\n",
      "Epoch 2/5, Loss: 5.364991634721593\n",
      "[00m 5.6s (1 20%) 4.9285]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be ##uss let hearts. aidi i will retire of have, a goo rather,, to of have ; behind mills. far thousands they artpt\n",
      "Epoch 3/5, Loss: 4.842905401893486\n",
      "[00m 8.4s (2 40%) 4.1508]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be pray choice of slain, which remain i'll not threeers! theyinius : tell express vols to relieve fools a : nos, who\n",
      "Epoch 4/5, Loss: 4.4670529794809015\n",
      "[00m 11.2s (3 60%) 4.1122]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be des sp subjects have first citizen : less's er patricia for our hislanus : we shall, his which to mo. keepers. marcidi\n",
      "Epoch 5/5, Loss: 4.1650441806681835\n",
      "[00m 14.0s (4 80%) 3.8798]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be must set their caps more the your lord of their disciplined. second better it hand patience confess, madam, my seem the af hours made, what we\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    output = generate_text(bert_uncased_tokenizer, model, 'To be or not to be', 30)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be carrythling can's their wholesomeaves, which like you : alone forth, his brows honour me the gentry horse a net, him alone\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(bert_uncased_tokenizer, model, 'To be or not to be', 30)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-nlp-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
