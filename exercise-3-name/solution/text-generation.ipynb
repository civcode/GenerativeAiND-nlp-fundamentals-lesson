{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = '../data/shakespeare_small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 50,085\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of characters: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "> - Skip?\n",
    "> - lowercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {\n",
    "    char: idx\n",
    "    for idx, (char, _) in enumerate(Counter(shakespeare_text).items())\n",
    "}\n",
    "\n",
    "idx2char = {\n",
    "    idx: char\n",
    "    for char, idx in char2idx.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [\n",
    "    char2idx[char] for char in shakespeare_text\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "> Skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(char2idx)\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, encoded_text: Sequence, sequence_length: int):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(\n",
    "            self.encoded_text[index: (index+self.sequence_length)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        # Target is shifted by one character\n",
    "        y = torch.tensor(\n",
    "            self.encoded_text[(index+1): (index+self.sequence_length+1)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, n_tokens: int, embedding_dim: int, hidden_dim: int):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "model = ShakespeareModel(n_tokens, embed_dim, hidden_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    input_str: str,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    generated_text = input_str.lower()\n",
    "    input_tensor = (\n",
    "        torch.tensor(\n",
    "            [char2idx[char] for char in input_str.lower()],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_chars):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text += idx2char[next_char_idx]\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_char_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "            \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since: float) -> str:\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.271035790138732\n",
      "[0m 6s (0 0%) 2.1613]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beses\n",
      "hathgr mysher splin aruso ant, waver generie-:\n",
      "male, anf you you indie, rour wayall your,\n",
      "carene\n",
      "Epoch 2/5, Loss: 1.9209989490600439\n",
      "[0m 12s (1 20%) 1.8412]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bety thatled as of it prowen hisous frught comenb the in the blous:\n",
      "and thest raga sabluft mised seggt\n",
      "Epoch 3/5, Loss: 1.8381634871418864\n",
      "[0m 18s (2 40%) 1.7816]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be from al propy trying\n",
      "ws\n",
      "youous, quarges,\n",
      "my madnty's and on't a grings sire in bear mesest enomant?\n",
      "Epoch 4/5, Loss: 1.7927972703696058\n",
      "[0m 25s (3 60%) 1.8368]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bense, virster obence home.\n",
      "\n",
      "menenius:\n",
      "whint they showe fach, and of thesey prpeyeat comen i shalt the\n",
      "Epoch 5/5, Loss: 1.7628117293214645\n",
      "[0m 31s (4 80%) 1.7623]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bed, bllobliedts am noblents lits manous; when mieb.\n",
      "the dols, dishodiring.\n",
      "as see truster to see hobs\n"
     ]
    }
   ],
   "source": [
    "PHRASE = 'To be or not to be'\n",
    "epochs = 5\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text(model, PHRASE, 100)\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be ive hawino,\n",
      "thasd stele a tike theme us meences.\n",
      "\n",
      "volomply to it frots throbes my torcher\n",
      "and chath\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(model, 'To be or not to be', 100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize (Choose)\n",
    "- Normalization\n",
    "- Pretokenization\n",
    "- Tokenize\n",
    "- Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "bert_model_name = 'bert-base-cased'\n",
    "bert_model_name_uncased = 'bert-base-uncased'\n",
    "\n",
    "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    bert_model_name_uncased,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(\n",
    "    tokenizer,\n",
    "    text: str,\n",
    ") -> list[str]:\n",
    "    max_seq_length = tokenizer.model_max_length\n",
    "    # Chunk the string so tokenizer can take in full input\n",
    "    chunks_generator = (\n",
    "        text[i:i+max_seq_length]\n",
    "        for i in range(0, len(text), max_seq_length)\n",
    "    )\n",
    "    # Special tokens to ignore\n",
    "    ignore_tokens = (\n",
    "        tokenizer.cls_token,\n",
    "    )\n",
    "    # Get list of tokens (one chunk at a time)\n",
    "    tokenized_text = [\n",
    "        token\n",
    "        for chunk in chunks_generator\n",
    "        for token in tokenizer(chunk).tokens()\n",
    "        if (\n",
    "            token not in ignore_tokens\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_text = (\n",
    "    tokenize_text(\n",
    "        tokenizer=bert_uncased_tokenizer,\n",
    "        text=raw_text,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {token: idx for idx, (token, _) in enumerate(Counter(shakespeare_text).items())}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "encoded = [token2idx[token] for token in shakespeare_text]\n",
    "n_tokens = len(token2idx)\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(token2idx)\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, encoded_text: Sequence, sequence_length: int):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(\n",
    "            self.encoded_text[index: (index+self.sequence_length)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        # Target is shifted by one token\n",
    "        y = torch.tensor(\n",
    "            self.encoded_text[(index+1): (index+self.sequence_length+1)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 16  # Tokens \n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, n_tokens: int, embedding_dim: int, hidden_dim: int):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "model = ShakespeareModel(n_tokens, embed_dim, hidden_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Genaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    input_str: str,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    tokenized_text = tokenize_text(tokenizer=tokenizer, text=input_str)\n",
    "    input_tensor = (\n",
    "        torch.tensor(\n",
    "            [token2idx[token] for token in tokenized_text],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    generated_text = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_tokens):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_token_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text.append(idx2token[next_token_idx])\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_token_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "    # Convert to text again\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(generated_text)\n",
    "    output_str = input_str + ' ' + tokenizer.decode(output_ids)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since: float) -> str:\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.166176602460336\n",
      "[0m 2s (0 0%) 5.4258]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be risen this ofecteni between, was do who brook foremost'let may ll the le common with i o he to what have letter., off\n",
      "Epoch 2/5, Loss: 5.406025084896364\n",
      "[0m 5s (1 20%) 5.2818]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be burn and tired shall these farewell ; ilina great, than, : no, i the be surf mal good art predecessors a sds by first!\n",
      "Epoch 3/5, Loss: 4.871879844849812\n",
      "[0m 8s (2 40%) 4.1865]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be has the ice [SEP] they? all : away even to lip, we are? upon him corn clap look to there'we ne be officers.\n",
      "Epoch 4/5, Loss: 4.4878922153786185\n",
      "[0m 10s (3 60%) 4.3801]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be ours to this teous that or strength. com. valeria : come, which herd tolean men charged - re behind fast you are we shall\n",
      "Epoch 5/5, Loss: 4.180407120409795\n",
      "[0m 13s (4 80%) 3.6587]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be org me to the officers and him guard might sicinius : here report? all - in arms. those that with if me, i replied\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    output = generate_text(bert_uncased_tokenizer, model, 'To be or not to be', 30)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be , and di at and poor! and ; sir, during the matter : t virgilia : i heard her would letter'instruments [SEP] times ; i\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(bert_uncased_tokenizer, model, 'To be or not to be', 30)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-nlp-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
