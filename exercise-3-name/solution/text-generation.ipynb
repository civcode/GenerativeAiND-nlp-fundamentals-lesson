{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "response = requests.get(shakespeare_url)\n",
    "raw_text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size of the dataset to make it manageable for smaller systems\n",
    "raw_text = raw_text[:50_085]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "> - Skip?\n",
    "> - lowercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {\n",
    "    char: idx\n",
    "    for idx, (char, _) in enumerate(Counter(shakespeare_text).items())\n",
    "}\n",
    "\n",
    "idx2char = {\n",
    "    idx: char\n",
    "    for char, idx in char2idx.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = [\n",
    "    char2idx[char] for char in shakespeare_text\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "> Skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(char2idx)\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, encoded_text, sequence_length):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(\n",
    "            self.encoded_text[index: (index+self.sequence_length)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        # Target is shifted by one character\n",
    "        y = torch.tensor(\n",
    "            self.encoded_text[(index+1): (index+self.sequence_length+1)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, n_tokens, embedding_dim, hidden_dim):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "model = ShakespeareModel(n_tokens, embed_dim, hidden_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    input_str,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_text = input_str.lower()\n",
    "    input_tensor = (\n",
    "        torch.tensor(\n",
    "            [char2idx[char] for char in input_str.lower()],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_chars):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text += idx2char[next_char_idx]\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_char_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "            \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.273213369244585\n",
      "[0m 6s (0 0%) 2.0410]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be. oon frilest snu:\n",
      "it pearc a visuse's lortifius: if rith\n",
      "turts. i hink,\n",
      "male miree he thet your wie\n",
      "Epoch 2/5, Loss: 1.908403696572057\n",
      "[0m 12s (1 20%) 1.9153]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besen tus for had mutingell you not rese'th in wirlion\n",
      "than one, pertiu,\n",
      "i hind noa dour but. oldion; \n",
      "Epoch 3/5, Loss: 1.8226989426932776\n",
      "[0m 18s (2 40%) 1.6688]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besevence, and tate whithey?\n",
      "\n",
      "vore the do senant pabky in beinger my snuse we promes; and poopl: keind\n",
      "Epoch 4/5, Loss: 1.7800110093320902\n",
      "[0m 25s (3 60%) 1.9462]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be in geish they mak's\n",
      "true. than.\n",
      "\n",
      "first chall ofner:\n",
      "and for fonest:\n",
      "the that dutus,\n",
      "and berold the \n",
      "Epoch 5/5, Loss: 1.7531729327223171\n",
      "[0m 31s (4 80%) 1.6333]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to belben's secen than thust agy therest everace: youe?\n",
      "\n",
      "firs as\n",
      "a menen it of flootrowen.\n",
      "\n",
      "menutes; shal\n"
     ]
    }
   ],
   "source": [
    "PHRASE = 'To be or not to be'\n",
    "epochs = 5\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text(model, PHRASE, 100)\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_str, num_chars=100):\n",
    "    model.eval()\n",
    "    generated_text = input_str.lower()\n",
    "    input_tensor = (\n",
    "        torch.tensor(\n",
    "            [char2idx[char] for char in input_str.lower()],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_chars):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(output[0, -1], dim=0)\n",
    "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text += idx2char[next_char_idx]\n",
    "            input_tensor = torch.cat([input_tensor, torch.tensor([[next_char_idx]], dtype=torch.long)], 1)\n",
    "            \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to bey-ver viray; the citizes on us to it for by ukes the had ham padican then nobtou, and not ben that t\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(model, 'To be or not to be', 100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-Based Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize (Choose)\n",
    "- Normalization\n",
    "- Pretokenization\n",
    "- Tokenize\n",
    "- Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "bert_model_name = 'bert-base-cased'\n",
    "bert_model_name_uncased = 'bert-base-uncased'\n",
    "\n",
    "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    bert_model_name_uncased,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13047 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = bert_uncased_tokenizer(raw_text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {token: idx for idx, (token, _) in enumerate(Counter(shakespeare_text).items())}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "encoded = [token2idx[token] for token in shakespeare_text]\n",
    "n_tokens = len(token2idx)\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(token2idx)\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, encoded_text, sequence_length):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(\n",
    "            self.encoded_text[index: (index+self.sequence_length)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        # Target is shifted by one character\n",
    "        y = torch.tensor(\n",
    "            self.encoded_text[(index+1): (index+self.sequence_length+1)],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 16  # Tokens \n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded, length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, n_tokens, embedding_dim, hidden_dim):\n",
    "        super(ShakespeareModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "model = ShakespeareModel(n_tokens, embed_dim, hidden_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    tokenizer,\n",
    "    model,\n",
    "    input_str,\n",
    "    num_tokens=100,\n",
    "    temperature=1.0,\n",
    "):\n",
    "    model.eval()\n",
    "    tokenized_text = tokenizer(input_str).tokens()\n",
    "    input_tensor = (\n",
    "        torch.tensor(\n",
    "            [token2idx[token] for token in tokenized_text],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    generated_text = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_tokens):\n",
    "            output = model(input_tensor.to(device))\n",
    "            probabilities = nn.functional.softmax(\n",
    "                output[0, -1] / temperature,\n",
    "                dim=0,\n",
    "            )\n",
    "            next_token_idx = torch.multinomial(probabilities, 1).item()\n",
    "            generated_text.append(idx2token[next_token_idx])\n",
    "            input_tensor = torch.cat(\n",
    "                [\n",
    "                    input_tensor,\n",
    "                    torch.tensor([[next_token_idx]], dtype=torch.long),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "    # Convert to text again\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(generated_text)\n",
    "    output_str = input_str + ' ' + tokenizer.decode(output_ids)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 6.167175212327172\n",
      "[0m 2s (0 0%) 5.9299]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be arms returned bae planted what : i thou trumpeter mine : ', and shall. by com think newly - rome the capitol showingg i fallria em\n",
      "Epoch 2/5, Loss: 5.37737688480639\n",
      "[0m 5s (1 20%) 5.2679]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be of ; the lowest : hehor drop to delay : you in com cr theyces'dus : wish. la chief to so thank elders general\n",
      "Epoch 3/5, Loss: 4.848724785973044\n",
      "[0m 7s (2 40%) 4.4150]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be my used to the tu loves thing, now his. for them off of! you have have me ; thought, but you, and, we\n",
      "Epoch 4/5, Loss: 4.4673008731767245\n",
      "[0m 10s (3 60%) 4.3671]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be - coming, bulk mu shown o, it messenger. menenius : not your imprs well, yet our indeed. co heaven.warded\n",
      "Epoch 5/5, Loss: 4.162529150060579\n",
      "[0m 13s (4 80%) 3.5745]\n",
      "------------------------------------------------------------------------\n",
      "To be or not to be the widows of saw the weep my justice a kind tongues of em arms, how thens ; sir, i your affection off to indeed as the king\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / epochs * 100, loss))\n",
    "    print('-'*72)\n",
    "    output = generate_text(bert_uncased_tokenizer, model, 'To be or not to be', 30)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be me yours. i can cannot infect grace marcius. aufidius : weten to'lltrele that you have bal myge ll\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(bert_uncased_tokenizer, model, 'To be or not to be', 30)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-nlp-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
